{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "210ff57a",
   "metadata": {},
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
    "\n",
    "Ans.The below code calculates the probability using the given values and prints the result in both decimal and percentage formats. Ensure that the values for P_A and P_B match the provided probabilities in your problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "346eb704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability that an employee is a smoker given that they use the health insurance plan is: 0.40 or 40.00%\n"
     ]
    }
   ],
   "source": [
    "# Given probabilities\n",
    "P_A = 0.70  # Probability of using the health insurance plan\n",
    "P_B = 0.40  # Probability of being a smoker\n",
    "\n",
    "# Calculate the probability of using the health insurance plan and being a smoker\n",
    "P_A_and_B = P_B * P_A\n",
    "\n",
    "# Calculate the probability of being a smoker given that the employee uses the health insurance plan\n",
    "P_B_given_A = P_A_and_B / P_A\n",
    "\n",
    "# Print the result\n",
    "print(f\"The probability that an employee is a smoker given that they use the health insurance plan is: {P_B_given_A:.2f} or {P_B_given_A * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f300e47a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cb17fdd",
   "metadata": {},
   "source": [
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "\n",
    "Ans. The Bernoulli Naive Bayes and Multinomial Naive Bayes are two variants of the Naive Bayes classifier, which is a probabilistic machine learning algorithm commonly used for classification tasks. The main difference between them lies in the type of data they are designed to handle.\n",
    "\n",
    "### Bernoulli Naive Bayes:\n",
    "\n",
    "**1. Data Type:**\n",
    "   - **Binary Data:** Bernoulli Naive Bayes is specifically designed for binary feature data, where each feature is a binary variable (0 or 1).\n",
    "\n",
    "**2. Feature Representation:**\n",
    "   - **Feature Presence/Absence:** It models whether a particular feature is present (1) or absent (0) in the document or instance.\n",
    "\n",
    "**3. Example Application:**\n",
    "   - **Text Classification:** Bernoulli Naive Bayes can be used for text classification tasks where the presence or absence of words in a document is considered.\n",
    "\n",
    "**4. Probability Calculation:**\n",
    "   - **Bernoulli Distribution:** Assumes that features are binary and uses the Bernoulli distribution to model the likelihood of feature occurrence.\n",
    "\n",
    "### Multinomial Naive Bayes:\n",
    "\n",
    "**1. Data Type:**\n",
    "   - **Count Data:** Multinomial Naive Bayes is suitable for data with discrete features, often representing counts or frequencies of events.\n",
    "\n",
    "**2. Feature Representation:**\n",
    "   - **Frequency Counts:** It models the distribution of the frequency of each feature (word) in the document or instance.\n",
    "\n",
    "**3. Example Application:**\n",
    "   - **Text Classification:** Commonly used for text classification tasks where the frequency of words in a document is considered.\n",
    "\n",
    "**4. Probability Calculation:**\n",
    "   - **Multinomial Distribution:** Assumes that features follow a multinomial distribution, meaning it models the probability of observing a particular word with its frequency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c633cc9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b6ca4d3",
   "metadata": {},
   "source": [
    "Q3. How does Bernoulli Naive Bayes handle missing values?\n",
    "\n",
    "Ans. Naive Bayes algorithms, including Bernoulli Naive Bayes, typically assume that features are conditionally independent given the class label. However, the specific handling of missing values can depend on the implementation and the strategy chosen by the practitioner.\n",
    "\n",
    "Here are some common approaches to handling missing values in Bernoulli Naive Bayes:\n",
    "\n",
    "### 1. Ignoring Missing Values:\n",
    "\n",
    "- **Default Behavior:**\n",
    "  - Many implementations of Naive Bayes, including scikit-learn's `BernoulliNB`, handle missing values by simply ignoring them during the computation of probabilities.\n",
    "\n",
    "- **Impact on Probability Calculation:**\n",
    "  - If a feature has missing values, it is essentially treated as if that feature did not occur in the instance. The probability of the feature given the class label is calculated based on the observed instances where the feature is present.\n",
    "\n",
    "### 2. Imputation:\n",
    "\n",
    "- **Imputing Missing Values:**\n",
    "  - Some practitioners choose to impute missing values before applying the Bernoulli Naive Bayes algorithm.\n",
    "\n",
    "- **Imputation Strategies:**\n",
    "  - Common imputation strategies include replacing missing values with the mean, median, or mode of the observed values for that feature.\n",
    "\n",
    "- **Impact on Model:**\n",
    "  - Imputing missing values can affect the distribution of feature values, potentially influencing the model's performance.\n",
    "\n",
    "### 3. Special Handling:\n",
    "\n",
    "- **Designating a Category for Missing Values:**\n",
    "  - Another approach is to designate a special category or value for missing values. For example, if features are binary (0 or 1), a missing value might be assigned a special code (e.g., -1) to indicate its absence.\n",
    "\n",
    "- **Probability Calculation:**\n",
    "  - The model can then be trained to consider this special category separately when calculating probabilities.\n",
    "\n",
    "### 4. Conditional Independence:\n",
    "\n",
    "- **Assumption of Conditional Independence:**\n",
    "  - The Naive Bayes algorithm assumes conditional independence between features given the class label. In the presence of missing values, this independence assumption may not hold if the missingness is related to other features.\n",
    "\n",
    "- **Impact on Results:**\n",
    "  - If the missingness is systematic and related to other features, it may introduce bias into the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb3af93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de6b4b02",
   "metadata": {},
   "source": [
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "\n",
    "Ans. Yes, Gaussian Naive Bayes can be used for multi-class classification. Gaussian Naive Bayes is an extension of the Naive Bayes algorithm that assumes the features follow a Gaussian (normal) distribution. It is particularly suitable for continuous data, and it can be applied to problems with multiple classes.\n",
    "\n",
    "In the context of multi-class classification, Gaussian Naive Bayes extends the binary classification capability to handle more than two classes. The algorithm assigns a probability distribution to each class based on the observed values of features in the training data.\n",
    "\n",
    "Here's a brief overview of how Gaussian Naive Bayes can be used for multi-class classification:\n",
    "\n",
    "### Training:\n",
    "\n",
    "1. **Estimate Class Priors:**\n",
    "   - Calculate the prior probability of each class based on the frequency of each class in the training dataset.\n",
    "\n",
    "2. **Estimate Class Means and Variances:**\n",
    "   - For each feature and each class, calculate the mean and variance of the observed values in the training data.\n",
    "\n",
    "### Prediction:\n",
    "\n",
    "1. **Calculate Class Probabilities:**\n",
    "   - For each class, use the Gaussian probability density function to calculate the likelihood of the observed feature values given the estimated mean and variance.\n",
    "\n",
    "2. **Multiply with Priors:**\n",
    "   - Multiply the likelihood by the prior probability of each class to obtain the unnormalized posterior probabilities.\n",
    "\n",
    "3. **Normalize Probabilities:**\n",
    "   - Normalize the unnormalized probabilities to obtain the final posterior probabilities for each class.\n",
    "\n",
    "4. **Predict Class:**\n",
    "   - Assign the class with the highest posterior probability as the predicted class for the given instance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09743e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b716724e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
